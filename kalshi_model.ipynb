{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 10-Year Treasury Yield Prediction Model\n",
    "# Target: Friday, December 12, 2025\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Kalshi API Setup\n",
    "\n",
    "First, we need to authenticate with the Kalshi API. You'll need:\n",
    "- API Key ID\n",
    "- API Secret Key\n",
    "\n",
    "Get these from your Kalshi account settings.\n",
    "\n",
    "**Important:** If you get a `TypeError` about `get_kalshi_headers()`, make sure to **re-run cell 2** to reload the updated function definitions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalshi API configuration set up\n"
     ]
    }
   ],
   "source": [
    "# Kalshi API Configuration\n",
    "KALSHI_API_KEY_ID = \"a5db4aab-8cd5-4040-a60b-1f366375f776\" \n",
    "KALSHI_API_SECRET = \"\"\"MIIEpAIBAAKCAQEA6OLMIXYpE4WftPKMVhm/DzM/+wJUAI/tvfAdS2l8Oq5b2U2B\n",
    "                    2KdFa1SMKN1horb9duBQcvM/X5gDQUjwzt2DPgS94sU15MAoD+K9tUcsXwLWWeWq\n",
    "                    gO7fW+D5sHIKdn3YjYAr6WfoIYitT/h1dUddln8dEpiJBiffpaYb7HSYpYCzYVEL\n",
    "                    iTFPx972ufw/nJeZWZN2Fa8kIgd3po6F36adPRwCmcCwmysBOGALT59qpbIeanOn\n",
    "                    aYWIz5jKDdNn6/+Zqs/E2lqde4rRRzOnDvJ1ZUoDAdVcsp8th/qUo17t7tQf/+yb\n",
    "                    sBfqt983g4PforxtW/gU3/jTOID6ducMZPLlaQIDAQABAoIBAA5aicW9zSe4kpCq\n",
    "                    BGLhwMHHx5E84jJYrKJwzeie4a4r0fOeQ2438EWxvXCSJUx/zJzkYKdjfUvy7nxt\n",
    "                    g14UqNw/mZG3biw631ByiePGG7xNZfdYRvG3oO+krNVzUFhf4EmBLvOtvx2izJ/Q\n",
    "                    F5k4/IUw/d87PiX+R8DWo9D0MymI6F8sXMOY9NPvG2evM8sD/tLtgO8sbFJsVr8i\n",
    "                    s1LfM1x1yBgnzMDHAUbep+seSYztxJLoLJvtqBQCSxvs/bUG8lXoslwgVbUCZPh3\n",
    "                    J8RT3gZOXSZAfgB1EeLeUK6060Wzw/olyBJ+UrOBRvZVTYGURBR0b4UH/HFsHu5P\n",
    "                    1sedybMCgYEA8mbUokhUkPMC2u4uycvLqY6tQ7b//QqwGtB4dU7dVudM2WA866vA\n",
    "                    qTcM+nWHhVaOrF3OJ+EyrOg0ZL0CHenK4tB0HXsMHgVrB6Or8zfD6AKWdZwWO2xk\n",
    "                    F6wEzbceLTEhXUZsdpKQJpVpoaNTpIYkICeUI2MeFVBg/WzPMSIKk2cCgYEA9fNP\n",
    "                    O8Hg0alXvHonMpV1CZxsQT73b1Ai20ATT3TmDwJgaXymk1gKlWNAxqBEe9EKsk2M\n",
    "                    Ut0wJZap+GGtB2KeUOAV1T5LMZ5cwgX8MB5DgGN4Dnh/T9wgVMCgFnKEh6l8H9Z1\n",
    "                    1Q1oTlVaSNpFa/oMunJilrIKU7/LPkWJZCJDjq8CgYEAy3+upSrJ7AJHdFqfZwQz\n",
    "                    T36bUQzR7bJKU8iOSBVUVn/KEXfszQEctjKkL6P5iOQ41NXPm8VAUM6EJcpTe59p\n",
    "                    l27e/FzvuEheKUSrt30qd52siMHoHC0L/p+ITNedet0TIIZdylGSuQGYc311W9rN\n",
    "                    ez0e7XJpsAUR18/ARYRJkqMCgYEA35NxKFpC+3RcHCpSrYntXXJDekA5/4cyWGpz\n",
    "                    41vZsjUv3VSBvBlhbZFPFgAkoQVTGuihscX9+X1vPynTd44vakd5sWIySoWxvnJg\n",
    "                    YIFyTSiev1DMMpVhEooUPLOXRBdOjUaP0L+iXOr76pP4XYJtxaMred+Ywa7sm8vs\n",
    "                    bAk1rmMCgYAECK+bMBOogcq42nNO0RfO3dIMIejYOxjxedFiQEPfhFYFsSZ3bKp9\n",
    "                    aqCYGXLb9aTFRjAVL0KHUzrTdeKWiXcBT5PsUURyMessgBFdn2yMJmN2cha2aYqy\n",
    "                    2JHy8or21n4qvQjozf175V73o9mMEHk2719xasJ7bpY9Yh+NpAefAQ==\n",
    "\"\"\" \n",
    "\n",
    "# Try both base URLs - Kalshi may use different endpoints\n",
    "KALSHI_BASE_URL = \"https://api.elections.kalshi.com/trade-api/v2\"\n",
    "# Alternative: \"https://trading-api.kalshi.com/v1\" or \"https://api.kalshi.com/trade-api/v2\"\n",
    "\n",
    "# Market ticker for 10-Year Treasury Yield on Dec 12, 2025\n",
    "# Note: Make sure this ticker exists and is correct\n",
    "TARGET_MARKET_TICKER = \"KXTNOTED-25DEC31-T3.93\" \n",
    "\n",
    "# Clean up the API secret (remove extra whitespace and newlines)\n",
    "KALSHI_API_SECRET_CLEANED = KALSHI_API_SECRET.strip().replace('\\n', '').replace(' ', '')\n",
    "\n",
    "def get_kalshi_headers(method, path, body=\"\"):\n",
    "    \"\"\"\n",
    "    Generate Kalshi API headers with signature-based authentication.\n",
    "    Kalshi uses: KALSHI-ACCESS-KEY, KALSHI-ACCESS-TIMESTAMP, KALSHI-ACCESS-SIGNATURE\n",
    "    Uses RSA-PSS signing as per Kalshi API documentation.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import base64\n",
    "    from cryptography.hazmat.primitives import hashes, serialization\n",
    "    from cryptography.hazmat.primitives.asymmetric import padding, rsa\n",
    "    from cryptography.hazmat.backends import default_backend\n",
    "    \n",
    "    # Get current timestamp in milliseconds\n",
    "    timestamp = str(int(time.time() * 1000))\n",
    "    \n",
    "    # Create the string to sign: method + path + timestamp + body\n",
    "    string_to_sign = f\"{method.upper()}\\n{path}\\n{timestamp}\\n{body}\"\n",
    "    \n",
    "    try:\n",
    "        # Try to load the private key - handle different formats\n",
    "        private_key = None\n",
    "        \n",
    "        # First, try if it already has BEGIN/END markers\n",
    "        if \"BEGIN\" in KALSHI_API_SECRET_CLEANED or \"BEGIN\" in KALSHI_API_SECRET:\n",
    "            try:\n",
    "                private_key = serialization.load_pem_private_key(\n",
    "                    KALSHI_API_SECRET.encode(),\n",
    "                    password=None,\n",
    "                    backend=default_backend()\n",
    "                )\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # If that didn't work, try wrapping it\n",
    "        if private_key is None:\n",
    "            try:\n",
    "                private_key_pem = f\"-----BEGIN PRIVATE KEY-----\\n{KALSHI_API_SECRET_CLEANED}\\n-----END PRIVATE KEY-----\"\n",
    "                private_key = serialization.load_pem_private_key(\n",
    "                    private_key_pem.encode(),\n",
    "                    password=None,\n",
    "                    backend=default_backend()\n",
    "                )\n",
    "            except Exception as e1:\n",
    "                # Try RSA format instead\n",
    "                try:\n",
    "                    private_key_pem = f\"-----BEGIN RSA PRIVATE KEY-----\\n{KALSHI_API_SECRET_CLEANED}\\n-----END RSA PRIVATE KEY-----\"\n",
    "                    private_key = serialization.load_pem_private_key(\n",
    "                        private_key_pem.encode(),\n",
    "                        password=None,\n",
    "                        backend=default_backend()\n",
    "                    )\n",
    "                except Exception as e2:\n",
    "                    raise Exception(f\"Failed to parse private key. PKCS8 error: {e1}, RSA error: {e2}\")\n",
    "        \n",
    "        # Sign the string using RSA-PSS (as per Kalshi docs)\n",
    "        signature = private_key.sign(\n",
    "            string_to_sign.encode(),\n",
    "            padding.PSS(\n",
    "                mgf=padding.MGF1(hashes.SHA256()),\n",
    "                salt_length=padding.PSS.MAX_LENGTH\n",
    "            ),\n",
    "            hashes.SHA256()\n",
    "        )\n",
    "        \n",
    "        # Encode signature in base64\n",
    "        signature_b64 = base64.b64encode(signature).decode()\n",
    "        \n",
    "        headers = {\n",
    "            \"KALSHI-ACCESS-KEY\": KALSHI_API_KEY_ID,\n",
    "            \"KALSHI-ACCESS-TIMESTAMP\": timestamp,\n",
    "            \"KALSHI-ACCESS-SIGNATURE\": signature_b64,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        return headers\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating signature: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Fallback: try simpler authentication (may work for some endpoints)\n",
    "        return {\n",
    "            \"KALSHI-ACCESS-KEY\": KALSHI_API_KEY_ID,\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "print(\"Kalshi API configuration set up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kalshi data fetching functions defined\n"
     ]
    }
   ],
   "source": [
    "# Function to fetch market data from Kalshi\n",
    "def get_kalshi_market_data(ticker, limit=1000):\n",
    "    \"\"\"\n",
    "    Tries multiple endpoints: trades (historical), then orderbook (current).\n",
    "    Returns: DataFrame with market data.\n",
    "    \"\"\"\n",
    "    import urllib.parse\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "\n",
    "    encoded_ticker = urllib.parse.quote(ticker, safe=\"\")\n",
    "\n",
    "    # endpoint_template is just a label now, not literally the path\n",
    "    endpoints_to_try = [\n",
    "        (\"trades\", \"trades\"),\n",
    "        (\"orderbook\", \"orderbook\"),\n",
    "    ]\n",
    "\n",
    "    for endpoint_template, endpoint_name in endpoints_to_try:\n",
    "        # ----- Build path + params depending on endpoint -----\n",
    "        if endpoint_name == \"trades\":\n",
    "            # /markets/trades?ticker=KXTNOTED-25DEC31-T3.93&limit=...\n",
    "            path = \"/markets/trades\"\n",
    "            params = {\"ticker\": ticker, \"limit\": limit}\n",
    "\n",
    "        elif endpoint_name == \"orderbook\":\n",
    "            # /markets/KXTNOTED-25DEC31-T3.93/orderbook\n",
    "            path = f\"/markets/{encoded_ticker}/orderbook\"\n",
    "            params = {}\n",
    "\n",
    "        # Build query string\n",
    "        query_string = urllib.parse.urlencode(params)\n",
    "        path_with_query = f\"{path}?{query_string}\" if query_string else path\n",
    "\n",
    "        url = f\"{KALSHI_BASE_URL}{path_with_query}\"\n",
    "        headers = get_kalshi_headers(\"GET\", path_with_query)\n",
    "\n",
    "        print(f\"Trying {endpoint_name} endpoint for market: {ticker}\")\n",
    "        print(\"URL:\", url)\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, headers=headers)\n",
    "        except Exception as e:\n",
    "            print(f\"{endpoint_name} request failed:\", e)\n",
    "            continue\n",
    "\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"✗ {endpoint_name} endpoint returned {resp.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = resp.json()\n",
    "\n",
    "        if endpoint_name == \"trades\":\n",
    "            records = data.get(\"trades\", [])\n",
    "        else:  # orderbook\n",
    "            records = [data]  # whatever structure you want\n",
    "\n",
    "        df = pd.DataFrame(records)\n",
    "        if not df.empty:\n",
    "            return df  # success, stop trying others\n",
    "\n",
    "    # If all endpoints fail:\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Function to get market orderbook (for probability distribution)\n",
    "def get_kalshi_orderbook(ticker):\n",
    "    \"\"\"Get current orderbook to extract probability distribution from ladder prices\"\"\"\n",
    "    import urllib.parse\n",
    "    \n",
    "    encoded_ticker = urllib.parse.quote(ticker, safe='')\n",
    "    path = f\"/markets/{encoded_ticker}/orderbook\"\n",
    "    url = f\"{KALSHI_BASE_URL}{path}\"\n",
    "    \n",
    "    headers = get_kalshi_headers(\"GET\", path)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching orderbook for market: {ticker}\")\n",
    "        print(f\"URL: {url}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"Successfully fetched orderbook\")\n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Error fetching orderbook: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception fetching orderbook: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Function to search for markets (to find the correct ticker)\n",
    "def search_kalshi_markets(query=None, limit=100):\n",
    "    \"\"\"Search for markets on Kalshi\"\"\"\n",
    "    import urllib.parse\n",
    "    \n",
    "    path = \"/markets\"\n",
    "    params = {\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    if query:\n",
    "        params[\"keyword\"] = query\n",
    "    \n",
    "    query_string = urllib.parse.urlencode(params)\n",
    "    path_with_query = f\"{path}?{query_string}\" if query_string else path\n",
    "    \n",
    "    url = f\"{KALSHI_BASE_URL}{path_with_query}\"\n",
    "    headers = get_kalshi_headers(\"GET\", path_with_query)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            markets = data.get('markets', [])\n",
    "            return pd.DataFrame(markets)\n",
    "        else:\n",
    "            print(f\"Error searching markets: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception searching markets: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to get a specific market by ticker\n",
    "def get_kalshi_market(ticker):\n",
    "    \"\"\"Get market information for a specific ticker\"\"\"\n",
    "    import urllib.parse\n",
    "    \n",
    "    encoded_ticker = urllib.parse.quote(ticker, safe='')\n",
    "    path = f\"/markets/{encoded_ticker}\"\n",
    "    url = f\"{KALSHI_BASE_URL}{path}\"\n",
    "    \n",
    "    headers = get_kalshi_headers(\"GET\", path)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching market info for: {ticker}\")\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"Successfully fetched market info\")\n",
    "            return data.get('market', data)\n",
    "        else:\n",
    "            print(f\"Error fetching market: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception fetching market: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Kalshi data fetching functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Fetch External Data from FRED\n",
    "\n",
    "FRED (Federal Reserve Economic Data) is free and has excellent Treasury yield data.\n",
    "You'll need a free API key from: https://fred.stlouisfed.org/docs/api/api_key.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRED data fetching functions defined\n"
     ]
    }
   ],
   "source": [
    "# FRED API Configuration\n",
    "FRED_API_KEY = \"bf64fd190ba3f88eee30224cded034ba\"  # Get free key from https://fred.stlouisfed.org/docs/api/api_key.html\n",
    "FRED_BASE_URL = \"https://api.stlouisfed.org/fred\"\n",
    "\n",
    "def get_fred_data(series_id, start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Fetch data from FRED API\n",
    "    \n",
    "    Key series IDs for Treasury yields:\n",
    "    - DGS10: 10-Year Treasury Constant Maturity Rate\n",
    "    - DGS2: 2-Year Treasury Constant Maturity Rate\n",
    "    - DGS30: 30-Year Treasury Constant Maturity Rate\n",
    "    - DFF: Federal Funds Rate\n",
    "    - CPIAUCSL: Consumer Price Index\n",
    "    - UNRATE: Unemployment Rate\n",
    "    \"\"\"\n",
    "    url = f\"{FRED_BASE_URL}/series/observations\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": FRED_API_KEY,\n",
    "        \"file_type\": \"json\",\n",
    "        \"sort_order\": \"asc\"\n",
    "    }\n",
    "    \n",
    "    if start_date:\n",
    "        params[\"observation_start\"] = start_date\n",
    "    if end_date:\n",
    "        params[\"observation_end\"] = end_date\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            observations = data.get('observations', [])\n",
    "            df = pd.DataFrame(observations)\n",
    "            if not df.empty:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "                df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "                df = df.dropna(subset=['value'])\n",
    "                df = df.rename(columns={'value': series_id})\n",
    "            return df[['date', series_id]]\n",
    "        else:\n",
    "            print(f\"Error fetching FRED data: {response.status_code}\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception fetching FRED data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"FRED data fetching functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Data Collection\n",
    "\n",
    "Let's fetch the historical data we need for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching 10-Year Treasury Yield data from FRED...\n",
      "Fetched 1481 observations\n",
      "\n",
      "Fetching additional economic indicators...\n",
      "Data fetching complete!\n"
     ]
    }
   ],
   "source": [
    "# Fetch historical 10-year Treasury yield from FRED (primary external dataset)\n",
    "print(\"Fetching 10-Year Treasury Yield data from FRED...\")\n",
    "df_10y = get_fred_data(\"DGS10\", start_date=\"2020-01-01\")\n",
    "print(f\"Fetched {len(df_10y)} observations\")\n",
    "\n",
    "# Fetch additional economic indicators for features\n",
    "print(\"\\nFetching additional economic indicators...\")\n",
    "df_2y = get_fred_data(\"DGS2\", start_date=\"2020-01-01\")  # 2-year yield (spread indicator)\n",
    "df_30y = get_fred_data(\"DGS30\", start_date=\"2020-01-01\")  # 30-year yield\n",
    "df_ffr = get_fred_data(\"DFF\", start_date=\"2020-01-01\")  # Federal Funds Rate\n",
    "df_cpi = get_fred_data(\"CPIAUCSL\", start_date=\"2020-01-01\")  # CPI (inflation proxy)\n",
    "\n",
    "print(\"Data fetching complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging external datasets...\n",
      "External dataset shape: (2164, 9)\n",
      "Date range: 2020-01-01 00:00:00 to 2025-12-03 00:00:00\n",
      "\n",
      "First few rows:\n",
      "        date  DGS10  DGS2  DGS30   DFF  CPIAUCSL  yield_spread_10y_2y  \\\n",
      "0 2020-01-01    NaN   NaN    NaN  1.55   259.127                  NaN   \n",
      "1 2020-01-02   1.88  1.58   2.33  1.55       NaN                 0.30   \n",
      "2 2020-01-03   1.80  1.53   2.26  1.55       NaN                 0.27   \n",
      "3 2020-01-04    NaN   NaN    NaN  1.55       NaN                  NaN   \n",
      "4 2020-01-05    NaN   NaN    NaN  1.55       NaN                  NaN   \n",
      "\n",
      "   yield_spread_30y_10y  cpi_yoy  \n",
      "0                   NaN      NaN  \n",
      "1                  0.45      NaN  \n",
      "2                  0.46      NaN  \n",
      "3                   NaN      NaN  \n",
      "4                   NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "# Merge all FRED data on date\n",
    "print(\"Merging external datasets...\")\n",
    "df_external = df_10y.copy()\n",
    "for df in [df_2y, df_30y, df_ffr, df_cpi]:\n",
    "    if not df.empty:\n",
    "        df_external = pd.merge(df_external, df, on='date', how='outer')\n",
    "\n",
    "df_external = df_external.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Calculate additional features\n",
    "if 'DGS2' in df_external.columns and 'DGS10' in df_external.columns:\n",
    "    df_external['yield_spread_10y_2y'] = df_external['DGS10'] - df_external['DGS2']\n",
    "if 'DGS30' in df_external.columns and 'DGS10' in df_external.columns:\n",
    "    df_external['yield_spread_30y_10y'] = df_external['DGS30'] - df_external['DGS10']\n",
    "if 'CPIAUCSL' in df_external.columns:\n",
    "    df_external['cpi_yoy'] = df_external['CPIAUCSL'].pct_change(12) * 100  # Year-over-year CPI\n",
    "\n",
    "print(f\"External dataset shape: {df_external.shape}\")\n",
    "print(f\"Date range: {df_external['date'].min()} to {df_external['date'].max()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df_external.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3b: Test Kalshi API Connection and Find Market\n",
    "\n",
    "Let's test the API connection and verify the market ticker exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Kalshi API connection...\n",
      "\n",
      "Fetching market info for: KXTNOTED-25DEC31-T3.93\n",
      "Successfully fetched market info\n",
      "✓ Market found: Will the yield of 10-year U.S. treasury notes be below 3.93 on Dec 31, 2025?\n",
      "  Ticker: KXTNOTED-25DEC31-T3.93\n",
      "  Status: active\n"
     ]
    }
   ],
   "source": [
    "# Test Kalshi API connection and verify market ticker\n",
    "print(\"Testing Kalshi API connection...\\n\")\n",
    "\n",
    "# First, try to get the market info to verify the ticker exists\n",
    "market_info = get_kalshi_market(TARGET_MARKET_TICKER)\n",
    "if market_info:\n",
    "    print(f\"✓ Market found: {market_info.get('title', 'N/A')}\")\n",
    "    print(f\"  Ticker: {market_info.get('ticker', TARGET_MARKET_TICKER)}\")\n",
    "    print(f\"  Status: {market_info.get('status', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"✗ Market '{TARGET_MARKET_TICKER}' not found or error accessing it\")\n",
    "    print(\"\\nTrying to search for Treasury-related markets...\")\n",
    "    treasury_markets = search_kalshi_markets(query=\"treasury\", limit=20)\n",
    "    if not treasury_markets.empty:\n",
    "        print(f\"\\nFound {len(treasury_markets)} Treasury-related markets:\")\n",
    "        print(treasury_markets[['ticker', 'title', 'status']].head(10))\n",
    "    else:\n",
    "        print(\"No Treasury markets found. Check your API credentials and base URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to fetch Kalshi market data...\n",
      "Trying trades endpoint for market: KXTNOTED-25DEC31-T3.93\n",
      "URL: https://api.elections.kalshi.com/trade-api/v2/markets/trades?ticker=KXTNOTED-25DEC31-T3.93&limit=1000\n",
      "Fetched 267 trades from Kalshi\n",
      "Using FRED data as primary dataset for now\n"
     ]
    }
   ],
   "source": [
    "# Fetch Kalshi market data (if available)\n",
    "# Note: You'll need to find the actual market ticker for this specific market\n",
    "print(\"Attempting to fetch Kalshi market data...\")\n",
    "# Uncomment and update ticker when you have it:\n",
    "kalshi_data = get_kalshi_market_data(TARGET_MARKET_TICKER)\n",
    "print(f\"Fetched {len(kalshi_data)} trades from Kalshi\")\n",
    "\n",
    "# For now, we'll work with FRED data and simulate Kalshi probability distribution later\n",
    "print(\"Using FRED data as primary dataset for now\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature Engineering\n",
    "\n",
    "Create features for the predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "def create_features(df):\n",
    "    \"\"\"Create time-based and lagged features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure we have the target variable\n",
    "    if 'DGS10' not in df.columns:\n",
    "        print(\"Warning: DGS10 not found in dataframe\")\n",
    "        return df\n",
    "    \n",
    "    # Time features\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    \n",
    "    # Lagged features (previous values)\n",
    "    for lag in [1, 5, 10, 20, 30]:\n",
    "        df[f'DGS10_lag_{lag}'] = df['DGS10'].shift(lag)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for window in [5, 10, 20, 30, 60]:\n",
    "        df[f'DGS10_ma_{window}'] = df['DGS10'].rolling(window=window).mean()\n",
    "        df[f'DGS10_std_{window}'] = df['DGS10'].rolling(window=window).std()\n",
    "    \n",
    "    # Momentum features\n",
    "    df['DGS10_change_1d'] = df['DGS10'].diff(1)\n",
    "    df['DGS10_change_5d'] = df['DGS10'].diff(5)\n",
    "    df['DGS10_change_20d'] = df['DGS10'].diff(20)\n",
    "    \n",
    "    # Volatility\n",
    "    df['DGS10_volatility'] = df['DGS10'].rolling(window=20).std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features\n",
    "df_features = create_features(df_external)\n",
    "print(f\"Feature dataset shape: {df_features.shape}\")\n",
    "print(f\"Number of features: {len(df_features.columns) - 2}\")  # Excluding 'date' and 'DGS10'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Build Predictive Model\n",
    "\n",
    "We'll use a Random Forest model for its balance of accuracy and interpretability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Remove rows with missing target or too many missing features\n",
    "df_model = df_features.dropna(subset=['DGS10']).copy()\n",
    "\n",
    "# Select feature columns (exclude date and target)\n",
    "feature_cols = [col for col in df_model.columns \n",
    "                if col not in ['date', 'DGS10'] and df_model[col].notna().sum() > len(df_model) * 0.5]\n",
    "\n",
    "# Fill remaining NaN values with forward fill then backward fill\n",
    "df_model[feature_cols] = df_model[feature_cols].ffill().bfill()\n",
    "\n",
    "# Final cleanup - remove any remaining NaN\n",
    "df_model = df_model.dropna(subset=feature_cols + ['DGS10'])\n",
    "\n",
    "print(f\"Final dataset shape: {df_model.shape}\")\n",
    "print(f\"Using {len(feature_cols)} features\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i}. {col}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "# Use 80% for training, 20% for testing\n",
    "split_idx = int(len(df_model) * 0.8)\n",
    "df_train = df_model.iloc[:split_idx].copy()\n",
    "df_test = df_model.iloc[split_idx:].copy()\n",
    "\n",
    "X_train = df_train[feature_cols]\n",
    "y_train = df_train['DGS10']\n",
    "X_test = df_test[feature_cols]\n",
    "y_test = df_test['DGS10']\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"Training date range: {df_train['date'].min()} to {df_train['date'].max()}\")\n",
    "print(f\"Test date range: {df_test['date'].min()} to {df_test['date'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Training MAE: {train_mae:.4f}%\")\n",
    "print(f\"Test MAE: {test_mae:.4f}%\")\n",
    "print(f\"Training RMSE: {train_rmse:.4f}%\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}%\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Make Prediction for December 12, 2025\n",
    "\n",
    "Now we'll use the model to predict the 10-year Treasury yield on the target date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction for December 12, 2025\n",
    "target_date = datetime(2025, 12, 12)\n",
    "\n",
    "# Get the most recent data point to use as base\n",
    "latest_data = df_model.iloc[-1].copy()\n",
    "\n",
    "# Create a prediction row with the latest available features\n",
    "# For future prediction, we'll use the most recent values and adjust time features\n",
    "pred_row = latest_data[feature_cols].copy()\n",
    "\n",
    "# Update time features for target date\n",
    "pred_row['year'] = target_date.year\n",
    "pred_row['month'] = target_date.month\n",
    "pred_row['day_of_week'] = target_date.weekday()\n",
    "pred_row['day_of_year'] = target_date.timetuple().tm_yday\n",
    "\n",
    "# Convert to DataFrame for prediction\n",
    "X_pred = pd.DataFrame([pred_row])\n",
    "\n",
    "# Make prediction\n",
    "predicted_yield = model.predict(X_pred)[0]\n",
    "\n",
    "print(f\"Predicted 10-Year Treasury Yield on {target_date.strftime('%B %d, %Y')}: {predicted_yield:.4f}%\")\n",
    "print(f\"\\nNote: This is a point estimate. We'll create a probability distribution next.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Extract Probability Distribution from Kalshi Ladder Prices\n",
    "\n",
    "The requirement is to produce a probability distribution based on Kalshi ladder prices.\n",
    "This function will extract probabilities from the orderbook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probability_distribution_from_kalshi(orderbook_data):\n",
    "    \"\"\"\n",
    "    Extract probability distribution from Kalshi orderbook ladder prices.\n",
    "    \n",
    "    Kalshi markets are typically structured as binary options with different strike prices.\n",
    "    The prices represent implied probabilities.\n",
    "    \"\"\"\n",
    "    if orderbook_data is None:\n",
    "        print(\"No orderbook data available\")\n",
    "        return None\n",
    "    \n",
    "    # Kalshi orderbook structure may vary - adjust based on actual API response\n",
    "    # Typically contains: yes_bids, yes_asks, no_bids, no_asks for each strike\n",
    "    probabilities = []\n",
    "    \n",
    "    # Example structure (adjust based on actual API):\n",
    "    # For each ladder rung/strike:\n",
    "    #   - yes_price represents probability of that outcome\n",
    "    #   - no_price = 1 - yes_price\n",
    "    \n",
    "    try:\n",
    "        # If orderbook has ladder structure\n",
    "        if 'ladder' in orderbook_data:\n",
    "            for rung in orderbook_data['ladder']:\n",
    "                strike = rung.get('strike', rung.get('outcome', None))\n",
    "                yes_price = rung.get('yes_bid', rung.get('yes_price', None))\n",
    "                if yes_price is not None:\n",
    "                    probabilities.append({\n",
    "                        'yield_level': strike,\n",
    "                        'probability': yes_price\n",
    "                    })\n",
    "        \n",
    "        # Alternative: if orderbook has separate yes/no markets\n",
    "        elif 'yes_bids' in orderbook_data or 'yes_asks' in orderbook_data:\n",
    "            # Calculate mid-price as probability estimate\n",
    "            yes_bid = orderbook_data.get('yes_bids', [{}])[0].get('price', 0) if orderbook_data.get('yes_bids') else 0\n",
    "            yes_ask = orderbook_data.get('yes_asks', [{}])[0].get('price', 1) if orderbook_data.get('yes_asks') else 1\n",
    "            prob = (yes_bid + yes_ask) / 2\n",
    "            probabilities.append({\n",
    "                'yield_level': 'target',\n",
    "                'probability': prob\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(probabilities)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting probabilities: {e}\")\n",
    "        return None\n",
    "\n",
    "# If you have Kalshi orderbook data, uncomment:\n",
    "# orderbook = get_kalshi_orderbook(TARGET_MARKET_TICKER)\n",
    "# prob_dist = extract_probability_distribution_from_kalshi(orderbook)\n",
    "# print(prob_dist)\n",
    "\n",
    "print(\"Probability distribution extraction function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simulated probability distribution based on model uncertainty\n",
    "# In practice, you'll replace this with actual Kalshi ladder prices\n",
    "\n",
    "def create_probability_distribution_from_model(predicted_yield, uncertainty=0.5):\n",
    "    \"\"\"\n",
    "    Create a probability distribution around the predicted yield.\n",
    "    In production, this should come from Kalshi ladder prices.\n",
    "    \"\"\"\n",
    "    # Generate yield levels around prediction\n",
    "    yield_levels = np.arange(predicted_yield - 2, predicted_yield + 2, 0.1)\n",
    "    \n",
    "    # Use normal distribution centered on prediction\n",
    "    probabilities = np.exp(-0.5 * ((yield_levels - predicted_yield) / uncertainty) ** 2)\n",
    "    probabilities = probabilities / probabilities.sum()  # Normalize\n",
    "    \n",
    "    prob_dist = pd.DataFrame({\n",
    "        'yield_level': yield_levels,\n",
    "        'probability': probabilities\n",
    "    })\n",
    "    \n",
    "    return prob_dist\n",
    "\n",
    "# Create distribution\n",
    "prob_dist = create_probability_distribution_from_model(predicted_yield, uncertainty=0.3)\n",
    "\n",
    "print(\"Probability Distribution (simulated - replace with Kalshi data):\")\n",
    "print(prob_dist.head(10))\n",
    "print(f\"\\nExpected value: {(prob_dist['yield_level'] * prob_dist['probability']).sum():.4f}%\")\n",
    "print(f\"Standard deviation: {np.sqrt(((prob_dist['yield_level'] - predicted_yield)**2 * prob_dist['probability']).sum()):.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the probability distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(prob_dist['yield_level'], prob_dist['probability'], linewidth=2, label='Probability Distribution')\n",
    "plt.axvline(predicted_yield, color='r', linestyle='--', linewidth=2, label=f'Predicted: {predicted_yield:.2f}%')\n",
    "plt.xlabel('10-Year Treasury Yield (%)', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.title('Probability Distribution for 10-Year Treasury Yield on Dec 12, 2025', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Model Evaluation and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Time series of actual vs predicted\n",
    "ax1 = axes[0]\n",
    "ax1.plot(df_test['date'], y_test, label='Actual', linewidth=2, alpha=0.7)\n",
    "ax1.plot(df_test['date'], y_test_pred, label='Predicted', linewidth=2, alpha=0.7)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('10-Year Treasury Yield (%)', fontsize=12)\n",
    "ax1.set_title('Model Performance: Actual vs Predicted (Test Set)', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "ax2.set_xlabel('Actual Yield (%)', fontsize=12)\n",
    "ax2.set_ylabel('Predicted Yield (%)', fontsize=12)\n",
    "ax2.set_title('Prediction Accuracy', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Final Summary\n",
    "\n",
    "## Key Results:\n",
    "1. **Model Type**: Random Forest Regressor\n",
    "2. **Primary External Data**: FRED (10-year, 2-year, 30-year yields, Fed Funds Rate, CPI)\n",
    "3. **Predicted Yield**: [See output above]\n",
    "4. **Probability Distribution**: [See visualization above]\n",
    "\n",
    "## Next Steps:\n",
    "1. **Get Kalshi API credentials** and update the authentication\n",
    "2. **Find the exact market ticker** for \"Treasury 10-Year Yield on Friday, December 12, 2025\"\n",
    "3. **Fetch actual Kalshi market data** and replace simulated probability distribution\n",
    "4. **Refine model** with additional features or different algorithms if needed\n",
    "5. **Write 1-page methodology report**\n",
    "\n",
    "## To Improve Accuracy:\n",
    "- Add more external datasets (CME futures, credit spreads, etc.)\n",
    "- Include macroeconomic forecasts\n",
    "- Use ensemble methods\n",
    "- Incorporate Kalshi market prices as features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key results for report\n",
    "results = {\n",
    "    'target_date': '2025-12-12',\n",
    "    'predicted_yield': float(predicted_yield),\n",
    "    'model_type': 'RandomForestRegressor',\n",
    "    'test_mae': float(test_mae),\n",
    "    'test_rmse': float(test_rmse),\n",
    "    'top_features': feature_importance.head(10).to_dict('records'),\n",
    "    'probability_distribution': prob_dist.to_dict('records')\n",
    "}\n",
    "\n",
    "# Save to JSON for easy access\n",
    "with open('model_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to model_results.json\")\n",
    "print(f\"\\n=== FINAL PREDICTION ===\")\n",
    "print(f\"10-Year Treasury Yield on December 12, 2025: {predicted_yield:.4f}%\")\n",
    "print(f\"Model Test MAE: {test_mae:.4f}%\")\n",
    "print(f\"Model Test RMSE: {test_rmse:.4f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
